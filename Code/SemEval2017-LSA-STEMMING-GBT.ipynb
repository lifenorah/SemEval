{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.spatial.distance import cosine\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readXML(path):\n",
    "    \"\"\"\n",
    "    Read XML file into a dictionary\n",
    "    \"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    dataset = pd.DataFrame(columns=['QID', 'QAID'], dtype=int)\n",
    "    \n",
    "    for Question in root:\n",
    "        QID = int(Question.get('QID'))\n",
    "        Qtext = Question.find('Qtext').text\n",
    "        \n",
    "        for QApair in Question.iter('QApair'): \n",
    "            QAID = int(QApair.get('QAID'))\n",
    "            QArel = QApair.get('QArel')\n",
    "            QAconf = QApair.get('QAconf')\n",
    "            QAquestion = QApair.find('QAquestion').text\n",
    "            QAanswer = QApair.find('QAanswer').text\n",
    "            \n",
    "            dataset = dataset.append({'QID': QID,\n",
    "                                    'QAID': QAID,\n",
    "                                    'Qtext': Qtext,\n",
    "                                    'QAquestion': QAquestion,\n",
    "                                    'QAanswer': QAanswer,\n",
    "                                    'QArel': 0 if QArel == 'I' else 1,\n",
    "                                    'QAconf': QAconf}, ignore_index=True)\n",
    "            \n",
    "    dataset.set_index(['QID', 'QAID'], inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pairwise(X, y):\n",
    "    \"\"\"Transforms data into pairs with balanced labels for ranking\n",
    "    Transforms a n-class ranking problem into a two-class classification\n",
    "    problem. Subclasses implementing particular strategies for choosing\n",
    "    pairs should override this method.\n",
    "    In this method, all pairs are choosen, except for those that have the\n",
    "    same target value. The output is an array of balanced classes, i.e.\n",
    "    there are the same number of -1 as +1\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The data\n",
    "    y : array, shape (n_samples,) or (n_samples, 2)\n",
    "        Target labels. If it's a 2D array, the second column represents\n",
    "        the grouping of samples, i.e., samples with different groups will\n",
    "        not be considered.\n",
    "    Returns\n",
    "    -------\n",
    "    X_trans : array, shape (k, n_feaures)\n",
    "        Data as pairs\n",
    "    y_trans : array, shape (k,)\n",
    "        Output class labels, where classes have values {-1, +1}\n",
    "    \"\"\"\n",
    "    X_new = []\n",
    "    y_new = []\n",
    "    y = np.asarray(y)\n",
    "    if y.ndim == 1:\n",
    "        y = np.c_[y, np.ones(y.shape[0])]\n",
    "    comb = itertools.combinations(range(X.shape[0]), 2)\n",
    "    for k, (i, j) in enumerate(comb):\n",
    "        if y[i, 0] == y[j, 0] or y[i, 1] != y[j, 1]:\n",
    "            # skip if same target or different group\n",
    "            continue\n",
    "        X_new.append(X[i] - X[j])\n",
    "        y_new.append(np.sign(y[i, 0] - y[j, 0]))\n",
    "        # output balanced classes\n",
    "        if y_new[-1] != (-1) ** k:\n",
    "            y_new[-1] = - y_new[-1]\n",
    "            X_new[-1] = - X_new[-1]\n",
    "    return np.asarray(X_new), np.asarray(y_new).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = readXML('../TRAIN/SemEval2016-Task3-CQA-MD-train.xml')\n",
    "test_dataset = readXML('../TEST/2017/SemEval2017-Task3-CQA-MD-test.xml')\n",
    "\n",
    "train_dataset = train_dataset.sort_index(level=0, ascending=[False, True])\n",
    "train_dataset = train_dataset.reset_index().drop_duplicates().set_index(['QID', 'QAID'])\n",
    "\n",
    "test_dataset = test_dataset.sort_index(level=0, ascending=[False, True])\n",
    "test_dataset = test_dataset.reset_index().drop_duplicates().set_index(['QID', 'QAID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem import ISRIStemmer\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = ISRIStemmer()\n",
    "    def __call__(self, doc):\n",
    "         return [self.wnl.stem(t) for t in wordpunct_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.1, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=False,\n",
       "...0, n_iter=3,\n",
       "       random_state=None, tol=0.0)), ('normalizer', Normalizer(copy=False, norm='l2'))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lsa = Pipeline([('tfidf', TfidfVectorizer(min_df=1, max_df=0.1, tokenizer=StemTokenizer(), stop_words=stopwords.words('arabic'), smooth_idf=False, sublinear_tf=True, norm='l2', max_features=1000)),\n",
    "                      ('lsa',  TruncatedSVD(n_components=900,n_iter=3)),\n",
    "                      ('normalizer', Normalizer(copy=False))])\n",
    "train_lsa.fit(list(set(train_dataset['Qtext'])))\n",
    "\n",
    "test_lsa = Pipeline([('tfidf', TfidfVectorizer(min_df=1, max_df=0.1, tokenizer=StemTokenizer(), stop_words=stopwords.words('arabic'), smooth_idf=False, sublinear_tf=True, norm='l2', max_features=1000)),\n",
    "                     ('lsa',  TruncatedSVD(n_components=900,n_iter=3)),\n",
    "                     ('normalizer', Normalizer(copy=False))])\n",
    "test_lsa.fit(list(set(test_dataset['Qtext'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query_vec = train_lsa.transform(train_dataset['Qtext'])\n",
    "train_question_vec = train_lsa.transform(train_dataset['QAquestion'] + train_dataset['QAanswer'])\n",
    "\n",
    "X_train = train_question_vec\n",
    "Y_train = np.c_[train_dataset['QAconf'].astype(np.float), train_dataset.reset_index()['QID']]\n",
    "\n",
    "X_valid = X_train[int(X_train.shape[0] * 0.8):]\n",
    "Y_valid = Y_train[int(Y_train.shape[0] * 0.8):]\n",
    "\n",
    "X_train = X_train[:int(X_train.shape[0] * 0.8)]\n",
    "Y_train = Y_train[:int(Y_train.shape[0] * 0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans, Y_train_tran = transform_pairwise(X_train, Y_train)\n",
    "X_valid_trans, Y_valid_trans = transform_pairwise(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train_trans, Y_train_tran)\n",
    "clf.score(X_valid_trans, Y_valid_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_vec = test_lsa.transform(test_dataset['Qtext'])\n",
    "test_question_vec = test_lsa.transform(test_dataset['QAquestion'] + test_dataset['QAanswer'])\n",
    "\n",
    "X_test = test_question_vec\n",
    "Y_test = test_dataset['QArel']\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "predictions_proba = np.dot(X_test, clf.coef_.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['Score'] = predictions_proba\n",
    "test_dataset['Relevance'] = predictions\n",
    "test_dataset['Rank'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAP(gold_dataset, pred_dataset, th=10):\n",
    "    dataset = pred_dataset.join(gold_dataset, lsuffix='_pred', rsuffix='_gold')[['Score_pred', 'Relevance_gold']].reset_index()\n",
    "    dataset = dataset.sort_values(['QID', 'Score_pred'], ascending=False)\n",
    "    dataset['Rank_pred'] = dataset.groupby('QID')['Score_pred'].rank(ascending=False)\n",
    "    dataset = dataset[dataset.Relevance_gold]\n",
    "    dataset = dataset[dataset.Rank_pred <= th]\n",
    "    dataset['Position'] = dataset.groupby('QID')['Rank_pred'].rank(ascending=True)\n",
    "    dataset['Precision'] = dataset.Position / dataset.Rank_pred\n",
    "    AP = dataset.groupby('QID')['Precision'].mean()\n",
    "    return round(AP.sum() / len(pred_dataset.groupby('QID')),4) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_dataset = pd.read_csv('../EVAL/SemEval2017-Task3-CQA-MD-test.xml.subtaskD.relevancy', sep='\\t',  names=['QID', 'QAID', 'Rank', 'Score', 'Relevance'], index_col=['QID', 'QAID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.770000000000003"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP(gold_dataset, test_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
