{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from keras.layers import Input, Merge\n",
    "from keras.layers.core import Activation, Dense, Dropout, Permute\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import add, concatenate, dot\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import h5py\n",
    "import PyArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load embeddings\n",
    "dictionary = pickle.load(open('dictionary.pic', 'rb'))\n",
    "reverse_dictionary = pickle.load(open('vocabulary.pic', 'rb'))\n",
    "embeddings = pickle.load(open('embeddings.pic', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(infile):\n",
    "    tree = ET.parse(infile)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    stories, questions, answers = [], [], []\n",
    "\n",
    "    for Question in root:\n",
    "        QID = int(Question.get('QID'))\n",
    "        Qtext = Question.find('Qtext').text\n",
    "\n",
    "        for QApair in Question.iter('QApair'): \n",
    "            QAID = int(QApair.get('QAID'))\n",
    "            QArel = QApair.get('QArel')\n",
    "            QAquestion = QApair.find('QAquestion').text\n",
    "            QAanswer = QApair.find('QAanswer').text\n",
    "\n",
    "            stories.append(Qtext)\n",
    "            questions.append(QAquestion)\n",
    "            answers.append(QArel)\n",
    "    return stories, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../TRAIN\"\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"SemEval2016-Task3-CQA-MD-train.xml\")\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"SemEval2017-Task3-CQA-MD-test.xml\")\n",
    "\n",
    "# get the data\n",
    "data_train = get_data(TRAIN_FILE)\n",
    "data_test = get_data(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(train_data, test_data):\n",
    "    counter = collections.Counter()\n",
    "    for stories, questions, answers in [train_data, test_data]:\n",
    "        for story in stories:\n",
    "            for sent in story:\n",
    "                for word in [w for w in sent.split() if w not in stopwords]:\n",
    "                    word = preprocessor.deNoise(word)\n",
    "                    counter[word.lower()] += 1\n",
    "        for question in questions:\n",
    "            for word in [w for w in question.split() if w not in stopwords]:\n",
    "                word = preprocessor.deNoise(word)\n",
    "                counter[word.lower()]+= 1\n",
    "        for answer in answers:\n",
    "            for word in [w for w in answer.split() if w not in stopwords]:\n",
    "                word = preprocessor.deNoise(word)\n",
    "                counter[word.lower()] += 1\n",
    "    # no OOV here because there are not too many words in dataset\n",
    "    word2idx = {w:(i+1) for i, (w, _) in enumerate(counter.most_common())}\n",
    "    word2idx[\"PAD\"] = 0\n",
    "    idx2word = {v:k for k, v in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maxlens(train_data, test_data):\n",
    "    story_maxlen, question_maxlen = 0, 0\n",
    "    for stories, questions, _ in [train_data, test_data]:\n",
    "        for story in stories:\n",
    "            story_len = 0\n",
    "            for sent in story:\n",
    "                swords = sent.split()\n",
    "                story_len += len(swords)\n",
    "            if story_len > story_maxlen:\n",
    "                story_maxlen = story_len\n",
    "        for question in questions:\n",
    "            question_len = len(question.split())\n",
    "            if question_len > question_maxlen:\n",
    "                question_maxlen = question_len\n",
    "    return story_maxlen, question_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, word2idx, story_maxlen, question_maxlen):\n",
    "    Xs, Xq, Y = [], [], []\n",
    "    stories, questions, answers = data\n",
    "    for story, question, answer in zip(stories, questions, answers):\n",
    "        xs = [[word2idx[preprocessor.deNoise(w.lower())] for w in s.split() if w not in stopwords] \n",
    "                                   for s in story]\n",
    "        xs = list(itertools.chain.from_iterable(xs))\n",
    "        xq = [word2idx[preprocessor.deNoise(w.lower())] for w in question.split() if w not in stopwords]\n",
    "        Xs.append(xs)\n",
    "        Xq.append(xq)\n",
    "        Y.append(0 if answer == 'I' else 1)\n",
    "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
    "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
    "           Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30411 12600\n",
      "vocab size: 83416\n",
      "story maxlen: 1223, question maxlen: 865\n",
      "(30411, 1223) (30411, 865) 30411 (12600, 1223) (12600, 865) 12600\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../TRAIN\"\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"SemEval2016-Task3-CQA-MD-train.xml\")\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"SemEval2017-Task3-CQA-MD-test.xml\")\n",
    "\n",
    "# get the data\n",
    "data_train = get_data(TRAIN_FILE)\n",
    "data_test = get_data(TEST_FILE)\n",
    "\n",
    "print(len(data_train[0]), len(data_test[0]))\n",
    "\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.readlines()\n",
    "        \n",
    "preprocessor = PyArabic.ArabicPreprocessor()\n",
    "\n",
    "# build vocabulary from all the data\n",
    "word2idx, idx2word = build_vocab(data_train, data_test)\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab size: {:d}\".format(len(word2idx)))\n",
    "\n",
    "# compute max sequence length for each entity\n",
    "story_maxlen, question_maxlen = get_maxlens(data_train, data_test)\n",
    "print(\"story maxlen: {:d}, question maxlen: {:d}\".format(story_maxlen, question_maxlen))\n",
    "\n",
    "# vectorize the data\n",
    "Xstrain, Xqtrain, Ytrain = vectorize(data_train, word2idx, story_maxlen, question_maxlen)\n",
    "Xstest, Xqtest, Ytest = vectorize(data_test, word2idx, story_maxlen, question_maxlen)\n",
    "\n",
    "print(Xstrain.shape, Xqtrain.shape, len(Ytrain), Xstest.shape, Xqtest.shape, len(Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_119 (InputLayer)           (None, 1223)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_120 (InputLayer)           (None, 865)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_87 (Sequential)       multiple              5338624     input_119[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "sequential_88 (Sequential)       (None, 865, 64)       5338624     input_120[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lstm_56 (LSTM)                   (None, 64)            33024       sequential_87[1][0]              \n",
      "                                                                   sequential_88[1][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)     (None, 128)           0           lstm_56[0][0]                    \n",
      "                                                                   lstm_56[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_37 (Dense)                 (None, 1)             129         concatenate_23[0][0]             \n",
      "====================================================================================================\n",
      "Total params: 10,710,401\n",
      "Trainable params: 10,710,401\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### define network\n",
    "EMBEDDING_SIZE = 64\n",
    "LATENT_SIZE = 32\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# placeholders\n",
    "original_sequence = Input((story_maxlen,))\n",
    "question_sequence = Input((question_maxlen,))\n",
    "\n",
    "# encoders\n",
    "\n",
    "# embed the original question into a sequence of vectors of size story_maxlen\n",
    "original_encoder = Sequential()\n",
    "original_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                              input_length=question_maxlen))\n",
    "\n",
    "original_encoded = original_encoder(original_sequence)\n",
    "question_encoded = question_encoder(question_sequence)\n",
    "\n",
    "shared_lstm = LSTM(64)\n",
    "\n",
    "encoded_a = shared_lstm(original_encoded)\n",
    "encoded_b = shared_lstm(question_encoded)\n",
    "\n",
    "merged_vector = concatenate([encoded_a, encoded_b], axis=1)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(merged_vector)\n",
    "\n",
    "model = Model(inputs=[original_sequence, question_sequence], outputs=predictions)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24328 samples, validate on 6083 samples\n",
      "Epoch 1/5\n",
      " 3200/24328 [==>...........................] - ETA: 4891s - loss: 0.6752 - acc: 0.6075"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit([Xstrain, Xqtrain], Ytrain, batch_size=BATCH_SIZE, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_split=0.2)\n",
    "                    \n",
    "# plot accuracy and loss plot\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "model.save('SemEval-MemNN-Model.h5')\n",
    "\n",
    "#save the weights\n",
    "model.save_weights('SemEval-MemNN-Weights.h5')\n",
    "\n",
    "#save the architecture\n",
    "model_json = model.to_json()\n",
    "with open(\"SemEval-MemNN-Arch.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
