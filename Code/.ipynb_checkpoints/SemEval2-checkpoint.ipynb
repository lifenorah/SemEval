{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import csv\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_csv(path, name, directory):\n",
    "    \"\"\"Read XML dataset\"\"\"\n",
    "    with zipfile.ZipFile(path, mode='r') as z:\n",
    "        with z.open([filename for filename in z.namelist() if filename[-4:] == '.xml'][0]) as f:\n",
    "            tree = ET.parse(f)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            filename = os.path.join(directory, name + '.csv')\n",
    "\n",
    "            with open(filename, 'wt', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile, delimiter=',')\n",
    "\n",
    "                for Question in root:\n",
    "                    QID = int(Question.get('QID'))\n",
    "                    Qtext = Question.find('Qtext').text\n",
    "\n",
    "                    for QApair in Question.iter('QApair'): \n",
    "                        QAID = int(QApair.get('QAID'))\n",
    "                        QArel = QApair.get('QArel')\n",
    "                        QAconf = float(QApair.get('QAconf'))\n",
    "                        QAquestion = QApair.find('QAquestion').text\n",
    "                        QAanswer = QApair.find('QAanswer').text\n",
    "                        \n",
    "                        writer.writerow([QID, Qtext, QAID, QArel, QAconf, QAquestion, QAanswer])\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_csv('D:\\PhD\\SemEval\\TRAIN\\SemEval2016-Task3-CQA-MD-train.xml.zip', 'train', 'temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCUMENT_LENGTH = 10\n",
    "EMBEDDING_SIZE = 50\n",
    "n_words = 0\n",
    "MAX_LABEL = 15\n",
    "WORDS_FEATURE = 'words'  # Name of the input words feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator_spec_for_softmax_classification(\n",
    "    logits, labels, mode):\n",
    "    \"\"\"Returns EstimatorSpec instance for softmax classification.\"\"\"\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'class': predicted_classes,\n",
    "                'prob': tf.nn.softmax(logits)\n",
    "            })\n",
    "\n",
    "    onehot_labels = tf.one_hot(labels, MAX_LABEL, 1, 0)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    eval_metric_ops = {\n",
    "      'accuracy': tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predicted_classes)\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_model(features, labels, mode):\n",
    "    \"\"\"A bag-of-words model. Note it disregards the word order in the text.\"\"\"\n",
    "    bow_column = tf.feature_column.categorical_column_with_identity(\n",
    "      WORDS_FEATURE, num_buckets=n_words)\n",
    "    bow_embedding_column = tf.feature_column.embedding_column(\n",
    "      bow_column, dimension=EMBEDDING_SIZE)\n",
    "    bow = tf.feature_column.input_layer(\n",
    "      features,\n",
    "      feature_columns=[bow_embedding_column])\n",
    "    logits = tf.layers.dense(bow, MAX_LABEL, activation=None)\n",
    "\n",
    "    return estimator_spec_for_softmax_classification(\n",
    "      logits=logits, labels=labels, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(features, labels, mode):\n",
    "    \"\"\"RNN model to predict from sequence of words to a class.\"\"\"\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(\n",
    "      features[WORDS_FEATURE], vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unstack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    _, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for softmax\n",
    "    # classification over output classes.\n",
    "    logits = tf.layers.dense(encoding, MAX_LABEL, activation=None)\n",
    "    return estimator_spec_for_softmax_classification(\n",
    "      logits=logits, labels=labels, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "    filename='temp/train.csv',\n",
    "    features_dtype=np.str,\n",
    "    target_dtype=np.float32,\n",
    "    target_column=4)\n",
    "\n",
    "x_train = pd.Series(training_set.data[:,1])\n",
    "y_train = pd.Series(training_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process vocabulary\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "  MAX_DOCUMENT_LENGTH)\n",
    "\n",
    "x_transform_train = vocab_processor.fit_transform(x_train)\n",
    "x_train = np.array(list(x_transform_train))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
