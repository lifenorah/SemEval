{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Embedding, LSTM, Merge, Flatten, dot, merge\n",
    "from keras.optimizers import RMSprop, Adadelta, Adam\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readXML(path):\n",
    "    \"\"\"\n",
    "    Read XML file into a Pandas DataFrame\n",
    "    \"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    dataset = pd.DataFrame(columns=['QID', 'QAID'], dtype=int)\n",
    "    \n",
    "    for Question in root:\n",
    "        QID = int(Question.get('QID'))\n",
    "        Qtext = Question.find('Qtext').text\n",
    "        \n",
    "        for QApair in Question.iter('QApair'): \n",
    "            QAID = int(QApair.get('QAID'))\n",
    "            QArel = QApair.get('QArel')\n",
    "            QAconf = QApair.get('QAconf')\n",
    "            QAquestion = QApair.find('QAquestion').text\n",
    "            QAanswer = QApair.find('QAanswer').text\n",
    "            \n",
    "            dataset = dataset.append({'QID': QID,\n",
    "                                    'QAID': QAID,\n",
    "                                    'Qtext': Qtext,\n",
    "                                    'QAquestion': QAquestion,\n",
    "                                    'QAanswer': QAanswer,\n",
    "                                    'QArel': 0 if QArel == 'I' else 1,\n",
    "                                    'QAconf': QAconf}, ignore_index=True)\n",
    "            \n",
    "    dataset.set_index(['QID', 'QAID'], inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = readXML('../TRAIN/SemEval2016-Task3-CQA-MD-train.xml')\n",
    "test_dataset = readXML('../TEST/2017/SemEval2017-Task3-CQA-MD-test.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 86378 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:33: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query_texts_train = train_dataset['Qtext']\n",
    "question_texts_train = train_dataset['QAquestion']\n",
    "labels_train = train_dataset['QArel']\n",
    "\n",
    "query_texts_test = test_dataset['Qtext']\n",
    "question_texts_test = test_dataset['QAquestion']\n",
    "labels_test = test_dataset['QArel']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(query_texts_train.tolist() + question_texts_train.tolist() + query_texts_test.tolist() + question_texts_test.tolist())\n",
    "\n",
    "query_sequences_train = tokenizer.texts_to_sequences(query_texts_train)\n",
    "question_sequences_train = tokenizer.texts_to_sequences(question_texts_train)\n",
    "\n",
    "query_sequences_test = tokenizer.texts_to_sequences(query_texts_test)\n",
    "question_sequences_test = tokenizer.texts_to_sequences(question_texts_test)\n",
    "\n",
    "query_maxlen = max(map(len, (x for x in query_sequences_train + query_sequences_test)))\n",
    "question_maxlen = max(map(len, (x for x in question_sequences_train + question_sequences_test)))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = max(query_maxlen, question_maxlen)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "query_data_train = pad_sequences(query_sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "question_data_train = pad_sequences(question_sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "query_data_test = pad_sequences(query_sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "question_data_test = pad_sequences(question_sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels_train = labels_train.reshape(-1, 1)\n",
    "labels_test = labels_test.reshape(-1, 1)\n",
    "\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * query_data_train.shape[0])\n",
    "\n",
    "query_x_train = query_data_train[:-nb_validation_samples]\n",
    "question_x_train = question_data_train[:-nb_validation_samples]\n",
    "y_train = labels_train[:-nb_validation_samples]\n",
    "query_x_val = query_data_train[-nb_validation_samples:]\n",
    "question_x_val = question_data_train[-nb_validation_samples:]\n",
    "y_val = labels_train[-nb_validation_samples:]\n",
    "query_x_test = query_data_test\n",
    "question_x_test = question_data_test\n",
    "y_test = labels_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "embeddings = pickle.load(open('embeddings.pic', 'rb'))\n",
    "dictionary = pickle.load(open('dictionary.pic', 'rb'))\n",
    "\n",
    "for word in dictionary.keys():\n",
    "    embeddings_index[word] = embeddings[dictionary[word]]\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "c:\\python35\\lib\\site-packages\\keras\\legacy\\layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only layers of same output shape can be merged using sum mode. Layer shapes: [(None, 282, 282), (None, 670, 670)]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-84a220adcf05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0minput_encoded_question\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_encoder_question\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_question\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_encoded_query\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_encoded_question\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\keras\\legacy\\layers.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(inputs, mode, concat_axis, dot_axes, output_shape, output_mask, arguments, name)\u001b[0m\n\u001b[0;32m    456\u001b[0m                             \u001b[0mnode_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mtensor_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m                             name=name)\n\u001b[0m\u001b[0;32m    459\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmerge_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\keras\\legacy\\layers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, mode, concat_axis, dot_axes, output_shape, output_mask, arguments, node_indices, tensor_indices, name)\u001b[0m\n\u001b[0;32m    109\u001b[0m             self._arguments_validation(layers, mode,\n\u001b[0;32m    110\u001b[0m                                        \u001b[0mconcat_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdot_axes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                                        node_indices, tensor_indices)\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0minput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\keras\\legacy\\layers.py\u001b[0m in \u001b[0;36m_arguments_validation\u001b[1;34m(self, layers, mode, concat_axis, dot_axes, node_indices, tensor_indices)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 raise ValueError('Only layers of same output shape can '\n\u001b[0;32m    154\u001b[0m                                  \u001b[1;34m'be merged using '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' mode. '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                                  'Layer shapes: %s' % input_shapes)\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'cos'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dot'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Only layers of same output shape can be merged using sum mode. Layer shapes: [(None, 282, 282), (None, 670, 670)]"
     ]
    }
   ],
   "source": [
    "# network definition\n",
    "\n",
    "input_query = Input(shape=(query_maxlen,))\n",
    "input_question = Input(shape=(question_maxlen,))\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_query = Sequential()\n",
    "input_encoder_query.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                              output_dim=query_maxlen))\n",
    "input_encoder_query.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_question = Sequential()\n",
    "input_encoder_question.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                              output_dim=question_maxlen))\n",
    "input_encoder_question.add(Dropout(0.3))\n",
    "\n",
    "input_encoded_query = input_encoder_query(input_query)\n",
    "input_encoded_question = input_encoder_question(input_question)\n",
    "\n",
    "match = dot([input_encoded_query, input_encoded_question], axes=(2, 2))\n",
    "match = LSTM(32)(match)\n",
    "match = Dropout(0.3)(match)\n",
    "match = Activation('sigmoid')(match)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model.fit([query_x_train, question_x_train], y_train,\n",
    "          batch_size=32,\n",
    "          epochs=120,\n",
    "          validation_data=([query_x_val, query_x_val], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Accuracy on training set: 23.04%\n",
      "* Accuracy on test set: 34.07%\n"
     ]
    }
   ],
   "source": [
    "# compute final accuracy on training and test sets\n",
    "pred = model.predict([query_x_test, question_x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
