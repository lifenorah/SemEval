{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to(path, name, directory):\n",
    "    \"\"\"Read XML dataset\"\"\"\n",
    "    with zipfile.ZipFile(path, mode='r') as z:\n",
    "        with z.open([filename for filename in z.namelist() if filename[-4:] == '.xml'][0]) as f:\n",
    "            tree = ET.parse(f)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            filename = os.path.join(directory, name + '.tfrecords')\n",
    "            print('Writing', filename)\n",
    "            writer = tf.python_io.TFRecordWriter(filename)\n",
    "            \n",
    "            for Question in root:\n",
    "                QID = int(Question.get('QID'))\n",
    "                Qtext = Question.find('Qtext').text\n",
    "\n",
    "                for QApair in Question.iter('QApair'): \n",
    "                    QAID = int(QApair.get('QAID'))\n",
    "                    QArel = QApair.get('QArel')\n",
    "                    QAconf = float(QApair.get('QAconf'))\n",
    "                    QAquestion = QApair.find('QAquestion').text\n",
    "                    QAanswer = QApair.find('QAanswer').text\n",
    "\n",
    "                    csv.\n",
    "                    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                        'QID': _int64_feature(QID),\n",
    "                        'Qtext': _bytes_feature(tf.compat.as_bytes(Qtext)),\n",
    "                        'QAID': _int64_feature(QAID),\n",
    "                        'QArel': _bytes_feature(tf.compat.as_bytes(QArel)),\n",
    "                        'QAconf': _float_feature(QAconf),\n",
    "                        'QAquestion': _bytes_feature(tf.compat.as_bytes(QAquestion)),\n",
    "                        'QAanswer': _bytes_feature(tf.compat.as_bytes(QAanswer))}))\n",
    "\n",
    "                    writer.write(example.SerializeToString())\n",
    "\n",
    "            writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'temp'\n",
    "name = 'train'\n",
    "\n",
    "if not os.path.exists(os.path.join(directory, name + '.tfrecords')):\n",
    "    convert_to('D:\\PhD\\SemEval\\TRAIN\\SemEval2016-Task3-CQA-MD-train.xml.zip', name, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCUMENT_LENGTH = 10\n",
    "EMBEDDING_SIZE = 50\n",
    "n_words = 0\n",
    "MAX_LABEL = 15\n",
    "WORDS_FEATURE = 'QAquestion'  # Name of the input words feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator_spec_for_softmax_classification(\n",
    "    logits, labels, mode):\n",
    "    \"\"\"Returns EstimatorSpec instance for softmax classification.\"\"\"\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'class': predicted_classes,\n",
    "                'prob': tf.nn.softmax(logits)\n",
    "            })\n",
    "\n",
    "    onehot_labels = tf.one_hot(labels, MAX_LABEL, 1, 0)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    eval_metric_ops = {\n",
    "      'accuracy': tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predicted_classes)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_model(features, labels, mode):\n",
    "    \"\"\"A bag-of-words model. Note it disregards the word order in the text.\"\"\"\n",
    "    bow_column = tf.feature_column.categorical_column_with_identity(\n",
    "      WORDS_FEATURE, num_buckets=n_words)\n",
    "    bow_embedding_column = tf.feature_column.embedding_column(\n",
    "      bow_column, dimension=EMBEDDING_SIZE)\n",
    "    bow = tf.feature_column.input_layer(\n",
    "      features,\n",
    "      feature_columns=[bow_embedding_column])\n",
    "    logits = tf.layers.dense(bow, MAX_LABEL, activation=None)\n",
    "\n",
    "    return estimator_spec_for_softmax_classification(\n",
    "      logits=logits, labels=labels, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(features, labels, mode):\n",
    "    \"\"\"RNN model to predict from sequence of words to a class.\"\"\"\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(\n",
    "      features[WORDS_FEATURE], vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unstack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    _, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for softmax\n",
    "    # classification over output classes.\n",
    "    logits = tf.layers.dense(encoding, MAX_LABEL, activation=None)\n",
    "    return estimator_spec_for_softmax_classification(\n",
    "      logits=logits, labels=labels, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    features = {}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"QID\"], tf.cast(parsed_features[\"Qtext\"], tf.string), parsed_features[\"QAID\"], parsed_features[\"QArel\"], parsed_features[\"QAconf\"], parsed_features[\"QAquestion\"], parsed_features[\"QAanswer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_input_fn():\n",
    "    filenames = [\"temp\\train.tfrecords\"]\n",
    "    dataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "\n",
    "    # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "    # protocol buffer, and perform any additional per-record preprocessing.\n",
    "    def parser(record):\n",
    "        keys_to_features = {\n",
    "            \"QID\": tf.FixedLenFeature((), tf.int64),\n",
    "            \"Qtext\": tf.FixedLenFeature((), tf.string),\n",
    "            \"QAID\": tf.FixedLenFeature((), tf.int64),\n",
    "            \"QArel\": tf.FixedLenFeature((), tf.string),\n",
    "            \"QAconf\": tf.FixedLenFeature((), tf.float32),\n",
    "            \"QAquestion\": tf.FixedLenFeature((), tf.string),\n",
    "            \"QAanswer\": tf.FixedLenFeature((), tf.string)\n",
    "        }\n",
    "        parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "        # Perform additional preprocessing on the parsed data.\n",
    "        #image = tf.decode_jpeg(parsed[\"image_data\"])\n",
    "        #image = tf.reshape(image, [299, 299, 1])\n",
    "        #label = tf.cast(parsed[\"label\"], tf.int32)\n",
    "        \n",
    "        QID = parsed[\"QID\"]\n",
    "        Qtext = parsed[\"Qtext\"]\n",
    "        QAID = parsed[\"QAID\"]\n",
    "        QArel = parsed[\"QArel\"]\n",
    "        QAconf = parsed[\"QAconf\"]\n",
    "        QAquestion = parsed[\"QAquestion\"]\n",
    "        QAanswer = parsed[\"QAanswer\"]\n",
    "        \n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"QID\": QID,\n",
    "            \"Qtext\": Qtext,\n",
    "            \"QAID\": QAID,\n",
    "            \"QArel\": QArel,\n",
    "            \"QAquestion\": QAquestion,\n",
    "            \"QAanswer\": QAanswer}, QAconf\n",
    "\n",
    "    # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "    # tensor for each example.\n",
    "    dataset = dataset.map(parser)\n",
    "    \n",
    "    #dataset = dataset.shuffle(buffer_size=10000)\n",
    "    #dataset = dataset.batch(32)\n",
    "    #dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    # `features` is a dictionary in which each value is a batch of values for\n",
    "    # that feature; `labels` is a batch of labels.\n",
    "    features, labels = iterator.get_next()\n",
    "    \n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "categorical_column_with_vocabulary_file() got an unexpected keyword argument 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-484447217669>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m feature_columns = [\n\u001b[1;32m----> 2\u001b[1;33m       tf.feature_column.categorical_column_with_vocabulary_file('QAquestion', shape=[1])]\n\u001b[0m\u001b[0;32m      3\u001b[0m regressor = tf.estimator.DNNRegressor(\n\u001b[0;32m      4\u001b[0m   feature_columns=feature_columns, hidden_units=[10, 10])\n\u001b[0;32m      5\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset_input_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: categorical_column_with_vocabulary_file() got an unexpected keyword argument 'shape'"
     ]
    }
   ],
   "source": [
    "feature_columns = [\n",
    "      tf.feature_column.categorical_column_with_vocabulary_file('QAquestion', shape=[1])]\n",
    "regressor = tf.estimator.DNNRegressor(\n",
    "  feature_columns=feature_columns, hidden_units=[10, 10])\n",
    "regressor.train(input_fn=dataset_input_fn, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.contrib.learn.datasets.base.load_csv_without_header()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
