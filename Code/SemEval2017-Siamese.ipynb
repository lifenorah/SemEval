{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Embedding, LSTM, Merge, Flatten\n",
    "from keras.optimizers import RMSprop, Adadelta, Adam\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readXML(path):\n",
    "    \"\"\"\n",
    "    Read XML file into a Pandas DataFrame\n",
    "    \"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    dataset = pd.DataFrame(columns=['QID', 'QAID'], dtype=int)\n",
    "    \n",
    "    for Question in root:\n",
    "        QID = int(Question.get('QID'))\n",
    "        Qtext = Question.find('Qtext').text\n",
    "        \n",
    "        for QApair in Question.iter('QApair'): \n",
    "            QAID = int(QApair.get('QAID'))\n",
    "            QArel = QApair.get('QArel')\n",
    "            QAconf = QApair.get('QAconf')\n",
    "            QAquestion = QApair.find('QAquestion').text\n",
    "            QAanswer = QApair.find('QAanswer').text\n",
    "            \n",
    "            dataset = dataset.append({'QID': QID,\n",
    "                                    'QAID': QAID,\n",
    "                                    'Qtext': Qtext,\n",
    "                                    'QAquestion': QAquestion,\n",
    "                                    'QAanswer': QAanswer,\n",
    "                                    'QArel': 0 if QArel == 'I' else 1,\n",
    "                                    'QAconf': QAconf}, ignore_index=True)\n",
    "            \n",
    "    dataset.set_index(['QID', 'QAID'], inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = readXML('../TRAIN/SemEval2016-Task3-CQA-MD-train.xml')\n",
    "test_dataset = readXML('../TEST/2017/SemEval2017-Task3-CQA-MD-test.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 86378 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:31: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query_texts_train = train_dataset['Qtext']\n",
    "question_texts_train = train_dataset['QAquestion']\n",
    "labels_train = train_dataset['QArel']\n",
    "\n",
    "query_texts_test = test_dataset['Qtext']\n",
    "question_texts_test = test_dataset['QAquestion']\n",
    "labels_test = test_dataset['QArel']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(query_texts_train.tolist() + question_texts_train.tolist() + query_texts_test.tolist() + question_texts_test.tolist())\n",
    "\n",
    "query_sequences_train = tokenizer.texts_to_sequences(query_texts_train)\n",
    "question_sequences_train = tokenizer.texts_to_sequences(question_texts_train)\n",
    "\n",
    "query_sequences_test = tokenizer.texts_to_sequences(query_texts_test)\n",
    "question_sequences_test = tokenizer.texts_to_sequences(question_texts_test)\n",
    "\n",
    "query_maxlen = max(map(len, (x for x in query_sequences_train + query_sequences_test)))\n",
    "question_maxlen = max(map(len, (x for x in question_sequences_train + question_sequences_test)))\n",
    "MAX_SEQUENCE_LENGTH = max(query_maxlen, question_maxlen)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "query_data_train = pad_sequences(query_sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "question_data_train = pad_sequences(question_sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "query_data_test = pad_sequences(query_sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "question_data_test = pad_sequences(question_sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels_train = labels_train.reshape(-1, 1)\n",
    "labels_test = labels_test.reshape(-1, 1)\n",
    "\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data_train.shape[0])\n",
    "\n",
    "query_x_train = query_data_train[:-nb_validation_samples]\n",
    "question_x_train = question_data_train[:-nb_validation_samples]\n",
    "y_train = labels_train[:-nb_validation_samples]\n",
    "query_x_val = query_data_train[-nb_validation_samples:]\n",
    "question_x_val = question_data_train[-nb_validation_samples:]\n",
    "y_val = labels_train[-nb_validation_samples:]\n",
    "query_x_test = query_data_test\n",
    "question_x_test = question_data_test\n",
    "y_test = labels_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "embeddings = pickle.load(open('embeddings.pic', 'rb'))\n",
    "dictionary = pickle.load(open('dictionary.pic', 'rb'))\n",
    "\n",
    "for word in dictionary.keys():\n",
    "    embeddings_index[word] = embeddings[dictionary[word]]\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    sequence_input = Input(shape=input_shape)\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    x = Dense(128, activation='relu')(embedded_sequences)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(input, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return labels[predictions.ravel() < 0.5].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "# network definition\n",
    "\n",
    "input_query = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "input_question = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                           EMBEDDING_DIM,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=MAX_SEQUENCE_LENGTH,\n",
    "                           trainable=True)\n",
    "\n",
    "encoded_query = embedding_layer(input_query)\n",
    "encoded_question = embedding_layer(input_question)\n",
    "\n",
    "shared_lstm = LSTM(10)\n",
    "\n",
    "processed_query = shared_lstm(encoded_query)\n",
    "processed_question = shared_lstm(encoded_question)\n",
    "\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_query, processed_question])\n",
    "malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]), output_shape=lambda x: (x[0][0], 1))([processed_query, processed_question])\n",
    "\n",
    "model = Model(inputs=[input_query, input_question], outputs=[malstm_distance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24329 samples, validate on 6082 samples\n",
      "Epoch 1/2\n",
      "24329/24329 [==============================] - 595s 24ms/step - loss: 0.1919 - acc: 0.7208 - val_loss: 0.2345 - val_acc: 0.6143\n",
      "Epoch 2/2\n",
      "24329/24329 [==============================] - 700s 29ms/step - loss: 0.1672 - acc: 0.7914 - val_loss: 0.2347 - val_acc: 0.6159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dff64bed68>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "# Adadelta optimizer, with gradient clipping by norm\n",
    "optimizer = Adam()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.fit([query_x_train, question_x_train], y_train,\n",
    "          batch_size=30,\n",
    "          epochs=2,\n",
    "          validation_data=([query_x_val, question_x_val], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Accuracy on training set: 23.04%\n",
      "* Accuracy on test set: 34.07%\n"
     ]
    }
   ],
   "source": [
    "# compute final accuracy on training and test sets\n",
    "pred = model.predict([query_x_train, question_x_train])\n",
    "train_accuracy = compute_accuracy(pred, y_train)\n",
    "pred = model.predict([query_x_test, question_x_test])\n",
    "test_accuracy = compute_accuracy(pred, y_test)\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * train_accuracy))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
