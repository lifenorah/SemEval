{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "from keras.layers import Input, Merge\n",
    "from keras.layers.core import Activation, Dense, Dropout, Permute\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import add, concatenate, dot\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "import PyArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load word2vec embeddings\n",
    "dictionary = pickle.load(open('dictionary.pic', 'rb'))\n",
    "reverse_dictionary = pickle.load(open('vocabulary.pic', 'rb'))\n",
    "embeddings = pickle.load(open('embeddings.pic', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stopwords list\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.readlines()\n",
    "        \n",
    "preprocessor = PyArabic.ArabicPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset from XML file\n",
    "def get_data(infile):\n",
    "    tree = ET.parse(infile)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    queries, questions, labels = [], [], []\n",
    "\n",
    "    for Question in root:\n",
    "        QID = int(Question.get('QID'))\n",
    "        Qtext = Question.find('Qtext').text\n",
    "\n",
    "        for QApair in Question.iter('QApair'): \n",
    "            QAID = int(QApair.get('QAID'))\n",
    "            QArel = QApair.get('QArel')\n",
    "            QAquestion = QApair.find('QAquestion').text\n",
    "            QAanswer = QApair.find('QAanswer').text\n",
    "\n",
    "            queries.append(Qtext)\n",
    "            questions.append(QAquestion)\n",
    "            labels.append(QArel)\n",
    "    return queries, questions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the query/question\n",
    "def vectorize(data):\n",
    "    queries, questions, labels = [], [], []\n",
    "    \n",
    "    q, qq, l = data\n",
    "    \n",
    "    for query, question, label in zip(q, qq, l):\n",
    "\n",
    "        queries.append([embeddings[dictionary[preprocessor.deNoise(w)]] for w in query.split() if w in dictionary and w not in stopwords])\n",
    "        questions.append([embeddings[dictionary[preprocessor.deNoise(w)]] for w in question.split() if w in dictionary and w not in stopwords])\n",
    "        labels.append(label)\n",
    "    \n",
    "    query_maxlen = max([len(q) for q in queries])\n",
    "    question_maxlen = max([len(q) for q in questions])\n",
    "    \n",
    "    return (pad_sequences(queries, maxlen=query_maxlen),\n",
    "            pad_sequences(questions, maxlen=question_maxlen),\n",
    "            np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7384, 212, 100) (7384, 956, 100) 7384\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../DEV\"\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"SemEval2016-Task3-CQA-MD-dev.xml\")\n",
    "\n",
    "# get the training data\n",
    "data_train = get_data(TRAIN_FILE)\n",
    "\n",
    "# vectorize the training data\n",
    "Xqtrain, Xqqtrain, Ytrain = vectorize(data_train)\n",
    "\n",
    "print(Xqtrain.shape, Xqqtrain.shape, len(Ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question_maxlen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-e7b4a40021f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# placeholders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0moriginal_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_maxlen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mquestion_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion_maxlen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# encoders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'question_maxlen' is not defined"
     ]
    }
   ],
   "source": [
    "### define network\n",
    "EMBEDDING_SIZE = 64\n",
    "LATENT_SIZE = 32\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# placeholders\n",
    "original_sequence = Input((Xqtrain.shape[1],))\n",
    "question_sequence = Input((Xqtrain.shape[1],))\n",
    "\n",
    "# encoders\n",
    "\n",
    "# embed the original question into a sequence of vectors of size story_maxlen\n",
    "original_encoder = Sequential()\n",
    "original_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                              input_length=question_maxlen))\n",
    "\n",
    "original_encoded = original_encoder(original_sequence)\n",
    "question_encoded = question_encoder(question_sequence)\n",
    "\n",
    "shared_lstm = LSTM(64)\n",
    "\n",
    "encoded_a = shared_lstm(original_encoded)\n",
    "encoded_b = shared_lstm(question_encoded)\n",
    "\n",
    "merged_vector = concatenate([encoded_a, encoded_b], axis=1)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(merged_vector)\n",
    "\n",
    "model = Model(inputs=[original_sequence, question_sequence], outputs=predictions)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = model.fit([Xstrain, Xqtrain], Ytrain, batch_size=BATCH_SIZE, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_split=0.2)\n",
    "                    \n",
    "# plot accuracy and loss plot\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "model.save('SemEval-MemNN-Model.h5')\n",
    "\n",
    "#save the weights\n",
    "model.save_weights('SemEval-MemNN-Weights.h5')\n",
    "\n",
    "#save the architecture\n",
    "model_json = model.to_json()\n",
    "with open(\"SemEval-MemNN-Arch.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'queries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1bee0b23b785>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'queries' is not defined"
     ]
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = 'text words from query 1 from from'\n",
    "queries.append([embeddings[dictionary[preprocessor.deNoise(w)]] for w in query.split() if w in dictionary and w not in stopwords])\n",
    "#query_maxlen = max(map(len, (x for x, _, _ in queries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(q) for q in queries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20968"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['from']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
