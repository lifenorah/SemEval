{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Embedding, LSTM, Merge, Flatten, dot, merge, Activation\n",
    "from keras.optimizers import RMSprop, Adadelta, Adam\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readXML(path):\n",
    "    \"\"\"\n",
    "    Read XML file into a Pandas DataFrame\n",
    "    \"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    dataset = pd.DataFrame(columns=['QID', 'QAID'], dtype=int)\n",
    "    \n",
    "    for Question in root:\n",
    "        QID = int(Question.get('QID'))\n",
    "        Qtext = Question.find('Qtext').text\n",
    "        \n",
    "        for QApair in Question.iter('QApair'): \n",
    "            QAID = int(QApair.get('QAID'))\n",
    "            QArel = QApair.get('QArel')\n",
    "            QAconf = QApair.get('QAconf')\n",
    "            QAquestion = QApair.find('QAquestion').text\n",
    "            QAanswer = QApair.find('QAanswer').text\n",
    "            \n",
    "            dataset = dataset.append({'QID': QID,\n",
    "                                    'QAID': QAID,\n",
    "                                    'Qtext': Qtext,\n",
    "                                    'QAquestion': QAquestion,\n",
    "                                    'QAanswer': QAanswer,\n",
    "                                    'QArel': 0 if QArel == 'I' else 1,\n",
    "                                    'QAconf': QAconf}, ignore_index=True)\n",
    "            \n",
    "    dataset.set_index(['QID', 'QAID'], inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = readXML('../TRAIN/SemEval2016-Task3-CQA-MD-train.xml')\n",
    "test_dataset = readXML('../TEST/2017/SemEval2017-Task3-CQA-MD-test.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 86378 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:33: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query_texts_train = train_dataset['Qtext']\n",
    "question_texts_train = train_dataset['QAquestion']\n",
    "labels_train = train_dataset['QArel']\n",
    "\n",
    "query_texts_test = test_dataset['Qtext']\n",
    "question_texts_test = test_dataset['QAquestion']\n",
    "labels_test = test_dataset['QArel']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(query_texts_train.tolist() + question_texts_train.tolist() + query_texts_test.tolist() + question_texts_test.tolist())\n",
    "\n",
    "query_sequences_train = tokenizer.texts_to_sequences(query_texts_train)\n",
    "question_sequences_train = tokenizer.texts_to_sequences(question_texts_train)\n",
    "\n",
    "query_sequences_test = tokenizer.texts_to_sequences(query_texts_test)\n",
    "question_sequences_test = tokenizer.texts_to_sequences(question_texts_test)\n",
    "\n",
    "query_maxlen = max(map(len, (x for x in query_sequences_train + query_sequences_test)))\n",
    "question_maxlen = max(map(len, (x for x in question_sequences_train + question_sequences_test)))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = max(query_maxlen, question_maxlen)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "query_data_train = pad_sequences(query_sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "question_data_train = pad_sequences(question_sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "query_data_test = pad_sequences(query_sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "question_data_test = pad_sequences(question_sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels_train = labels_train.reshape(-1, 1)\n",
    "labels_test = labels_test.reshape(-1, 1)\n",
    "\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * query_data_train.shape[0])\n",
    "\n",
    "query_x_train = query_data_train[:-nb_validation_samples]\n",
    "question_x_train = question_data_train[:-nb_validation_samples]\n",
    "y_train = labels_train[:-nb_validation_samples]\n",
    "query_x_val = query_data_train[-nb_validation_samples:]\n",
    "question_x_val = question_data_train[-nb_validation_samples:]\n",
    "y_val = labels_train[-nb_validation_samples:]\n",
    "query_x_test = query_data_test\n",
    "question_x_test = question_data_test\n",
    "y_test = labels_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "embeddings = pickle.load(open('embeddings.pic', 'rb'))\n",
    "dictionary = pickle.load(open('dictionary.pic', 'rb'))\n",
    "\n",
    "for word in dictionary.keys():\n",
    "    embeddings_index[word] = embeddings[dictionary[word]]\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network definition\n",
    "\n",
    "input_query = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "input_question = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_query = Sequential()\n",
    "input_encoder_query.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                              output_dim=MAX_SEQUENCE_LENGTH))\n",
    "input_encoder_query.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_question = Sequential()\n",
    "input_encoder_question.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                              output_dim=MAX_SEQUENCE_LENGTH))\n",
    "input_encoder_question.add(Dropout(0.3))\n",
    "\n",
    "input_encoded_query = input_encoder_query(input_query)\n",
    "input_encoded_question = input_encoder_question(input_question)\n",
    "\n",
    "match = dot([input_encoded_query, input_encoded_question], axes=(2, 2))\n",
    "match = LSTM(10)(match)\n",
    "match = Dropout(0.3)(match)\n",
    "match = Activation('sigmoid')(match)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_query, input_question], match)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24329 samples, validate on 6082 samples\n",
      "Epoch 1/2\n",
      "24329/24329 [==============================] - 3044s 125ms/step - loss: 1.6810 - acc: 0.4380 - val_loss: 1.6354 - val_acc: 0.4078\n",
      "Epoch 2/2\n",
      "24329/24329 [==============================] - 3063s 126ms/step - loss: 1.6618 - acc: 0.4467 - val_loss: 1.6141 - val_acc: 0.4055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f242a71278>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model.fit([query_x_train, question_x_train], y_train,\n",
    "          batch_size=30,\n",
    "          epochs=2,\n",
    "          validation_data=([query_x_val, query_x_val], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute final accuracy on training and test sets\n",
    "pred = model.predict([query_x_test, question_x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12600, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
