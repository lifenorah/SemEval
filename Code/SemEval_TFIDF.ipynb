{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import csv\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readXML(path):\n",
    "    \"\"\"\n",
    "    Read XML file into a dictionary\n",
    "    \"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    dataset = {}\n",
    "    \n",
    "    for Question in root:\n",
    "        QID = int(Question.get('QID'))\n",
    "        Qtext = Question.find('Qtext').text\n",
    "        \n",
    "        dataset[QID] = {}\n",
    "        dataset[QID]['Qtext'] = Qtext\n",
    "        dataset[QID]['QApairs'] = {}\n",
    "        \n",
    "        for QApair in Question.iter('QApair'): \n",
    "            QAID = int(QApair.get('QAID'))\n",
    "            QArel = QApair.get('QArel')\n",
    "            QAquestion = QApair.find('QAquestion').text\n",
    "            QAanswer = QApair.find('QAanswer').text\n",
    "            \n",
    "            QQ = Qtext + QAquestion\n",
    "            \n",
    "            dataset[QID]['QApairs'][QAID] = {\n",
    "                'QAquestion': QAquestion,\n",
    "                'QAanswer': QAanswer,\n",
    "                'QArel': QArel,\n",
    "                'QQ': QQ}\n",
    "    return dataset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords-old.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import ArabicStemmer\n",
    "\n",
    "stemmer = ArabicStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        try:\n",
    "            stemmed.append(stemmer.stem(item))\n",
    "        except:\n",
    "            stemmed.append(item)\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tfidf(corpora):\n",
    "    tfidf_vect = TfidfVectorizer(stop_words=stopwords, max_features=10000)\n",
    "    tfidf_vect.fit(corpora)\n",
    "    return tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(dataset):\n",
    "    queries = [q['Qtext'] for q in dataset.values()]\n",
    "    questions = [pair['QAquestion'] for qid in dataset.keys() for pair in dataset[qid]['QApairs'].values()]\n",
    "    answers = [pair['QAanswer'] for qid in dataset.keys() for pair in dataset[qid]['QApairs'].values()]\n",
    "    relevancies = [0 if pair['QArel'] == 'I' else 1 for qid in dataset.keys() for pair in dataset[qid]['QApairs'].values()]\n",
    "    return queries, questions, answers, relevancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = readXML('../TRAIN/SemEval2016-Task3-CQA-MD-train.xml')\n",
    "testset = readXML('../TEST/2017/SemEval2017-Task3-CQA-MD-test.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train, questions_train, answers_train, relevancies_train = fetch_data(trainset)\n",
    "queries_test, questions_test, answers_test, relevancies_test = fetch_data(testset)\n",
    "\n",
    "tfidf_vect_train = vectorize_tfidf(queries_train + questions_train + answers_train)\n",
    "tfidf_vect_test = vectorize_tfidf(queries_test + questions_test + answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = tfidf_vect_train.transform(questions_train)\n",
    "target_train = relevancies_train\n",
    "data_test = tfidf_vect_test.transform(questions_test)\n",
    "target_test = relevancies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30411x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 785034 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"Decision Tree\",\n",
    "         \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "        \"Ridge Classifier\", \"Perceptron\", \"Passive-Aggressive\",\n",
    "        \"LinearSVC\", \"l2\", \"elasticnet\", \"Rocchio classifier\", \"MultinomialNB\", \"BernoulliNB\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(10),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    RidgeClassifier(tol=1e-2, solver=\"lsqr\"),\n",
    "    Perceptron(n_iter=50),\n",
    "    PassiveAggressiveClassifier(n_iter=50),\n",
    "    LinearSVC(penalty=\"l2\", dual=False, tol=1e-3),\n",
    "    SGDClassifier(alpha=.0001, n_iter=50, penalty=\"l2\"),\n",
    "    SGDClassifier(alpha=.0001, n_iter=50, penalty=\"elasticnet\"),\n",
    "    NearestCentroid(),\n",
    "    MultinomialNB(alpha=.01),\n",
    "    BernoulliNB(alpha=.01)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors 0.394801685081\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-75e54f3c95ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtarget_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    274\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m                 \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m                 random_seed)\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\libsvm_sparse.pyx\u001b[0m in \u001b[0;36msklearn.svm.libsvm_sparse.libsvm_sparse_train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;34m\"\"\"base matrix class for compressed row and column oriented matrices\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(data_train, target_train)\n",
    "    print(name, np.mean(clf.predict(data_test) == target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "\n",
    "svd = TruncatedSVD(128)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd)\n",
    "\n",
    "lsa_train = lsa.fit_transform(data_train)\n",
    "lsa_test = lsa.fit_transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(lsa_train, target_train)\n",
    "    print(name, np.mean(clf.predict(lsa_test) == target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron 0.607662347985\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(32)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd)\n",
    "\n",
    "lsa_train = lsa.fit_transform(data_train)\n",
    "lsa_test = lsa.fit_transform(data_test)\n",
    "\n",
    "clf = Perceptron(max_iter=50)\n",
    "clf.fit(lsa_train, target_train)\n",
    "print('Perceptron', np.mean(clf.predict(lsa_test) == target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=256, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=64, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron 0.607662347985\n"
     ]
    }
   ],
   "source": [
    "nmf_train = nmf.transform(data_train)\n",
    "nmf_test = nmf.transform(data_test)\n",
    "\n",
    "clf = Perceptron(max_iter=50)\n",
    "clf.fit(nmf_train, target_train)\n",
    "print('Perceptron', np.mean(clf.predict(nmf_test) == target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=128, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0).fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron 0.607662347985\n"
     ]
    }
   ],
   "source": [
    "lda_train = lda.transform(data_train)\n",
    "lda_test = lda.transform(data_test)\n",
    "\n",
    "clf = Perceptron(max_iter=50)\n",
    "clf.fit(lda_train, target_train)\n",
    "print('Perceptron', np.mean(clf.predict(lda_test) == target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6076623479850568"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(target_test) - 4936)/len(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(lda_train, target_train)\n",
    "    print(name, np.mean(clf.predict(lda_test) == target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.33628772])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:806: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (previous_error - error) / error_at_init < tol:\n",
      "c:\\python35\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:1035: ConvergenceWarning: Maximum number of iteration 1000 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "test_dataset_path = '../TEST/2017/SemEval2017-Task3-CQA-MD-test-input.xml'\n",
    "\n",
    "tree = ET.parse(test_dataset_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "    \n",
    "for Question in root:\n",
    "    QID = int(Question.get('QID'))\n",
    "    Qtext = Question.find('Qtext').text\n",
    "    \n",
    "    for QApair in Question.iter('QApair'): \n",
    "        QAID = int(QApair.get('QAID'))\n",
    "        QArel = QApair.get('QArel')\n",
    "        QAquestion = QApair.find('QAquestion').text\n",
    "        QAanswer = QApair.find('QAanswer').text\n",
    "        \n",
    "        query_data_test = tfidf_vect_test.transform([QAquestion])\n",
    "        query_lda_test =  lda.transform(query_data_test)\n",
    "        \n",
    "        query_nmf_test =  nmf.transform(query_data_test)\n",
    "        \n",
    "        \n",
    "        QArel = clf.predict(query_lsa_test)[0]\n",
    "        QAconf = clf.decision_function(query_lsa_test)[0]\n",
    "        \n",
    "        QApair.set('QArel', 'R' if QArel == 1 else 'I')\n",
    "        QApair.set('QAconf', str(round(QAconf, 4)))\n",
    "\n",
    "tree.write('../TEST/2017/SemEval2017-Task3-CQA-MD-test-input-tfidf-perceptron.xml', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(gold_path, pred_path):\n",
    "    \"\"\"\n",
    "    Measure MAP 'Mean Average Precision' using the gold labels and prediction labels\n",
    "    for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    gold_dataset = readXML(gold_path)\n",
    "    pred_dataset = readXML(pred_path)\n",
    "    \n",
    "    AP = []\n",
    "    \n",
    "    for QID in gold_dataset.keys():\n",
    "        assert gold_dataset[QID]['QApairs'].keys() == pred_dataset[QID]['QApairs'].keys()\n",
    "        gold_labels = [0 if QApair['QArel'] == 'I' else 1 for QApair in gold_dataset[QID]['QApairs'].values()]\n",
    "        pred_labels = [0 if QApair['QArel'] == 'I' else 1 for QApair in pred_dataset[QID]['QApairs'].values()]\n",
    "        \n",
    "        AP.append(average_precision_score(gold_labels, pred_labels))\n",
    "        \n",
    "        \n",
    "    MAP = np.nanmean(AP)\n",
    "    \n",
    "    return MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\sklearn\\metrics\\ranking.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48822222222222222"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map('../TEST/2017/SemEval2017-Task3-CQA-MD-test.xml', '../TEST/2017/SemEval2017-Task3-CQA-MD-test-input-tfidf-perceptron.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
